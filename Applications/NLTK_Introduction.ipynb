{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK : Natural Language ToolKit\n",
    "\n",
    "Email : <a href='mailto:madani.a@ucd.ac.ma'>madani.a@ucd.ac.ma</a>\n",
    "<img src='images/python.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qu'est-ce que c'est NLTK ?\n",
    "<p>\n",
    "Natural Language Toolkit (NLTK) est une boîte-à-outil permettant la création de programmes pour l'analyse de texte (TextMining). Cet ensemble a été créé à l'origine par Steven Bird et Edward Loper, en relation avec des cours de linguistique informatique à l'Université de Pennsylvanie en 2001.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travailler avec NLTK\n",
    "<p>\n",
    "La première chose à faire pour utiliser NLTK est de télécharger ce qui se nomme le <strong>NLTK corpora</strong>. On peut télécharger tout le Corpus. C'est vrai que c'est énorme (à peu près 10,9 Go), mais nous ne le ferons qu'une seule fois. Si vous connaissez déjà quel corpus vous utiliserez, inutile de télécharger cet ensemble.\n",
    "</p>\n",
    "<p>\n",
    "Dans votre éditeur Python IDLE, écrivez ceci :\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\prof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Pour télécharger le corpus stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "# Pour télécharger tout le corpus\n",
    "nltk.download()\n",
    "# Pour télécharger seulement le corpus de 'stopwords'\n",
    "#nltk.download('stopwords')\n",
    "# Pour télécharger seulement le corpus de 'tokenizer'\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "<p>\n",
    "Parfois, nous avons besoin d'éliminer des éléments inutiles afin que les données soient davatange traduisables pour l'ordinateur. En NLP, de telles données (des mots, words) sont qualifiées par <strong>stop words</strong>. Par conséquent, ces mots n'ont aucune signification pour nous, et nous souhaiterions les retirer.\n",
    "</p>\n",
    "<p>\n",
    "La libraire NLTK contient quelques stopwords. Pour les connaître, écrivons ces petits scripts :\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'yours', 'their', \"doesn't\", 'again', 'd', 'hers', 'very', 'my', 'more', 'm', 'wasn', 'his', 'further', 'just', 'we', 'doing', 'myself', 'yourselves', 'so', \"you'll\", 'and', 'down', 'both', 'was', 'does', 'through', 'ain', 'during', 'him', 'shan', \"you've\", 'up', 'those', \"isn't\", 'where', 'ourselves', 'only', 'under', 'a', 'that', 'own', \"don't\", 'should', 'am', 'it', 'ours', 'all', 'after', 'shouldn', 'hasn', 'wouldn', 'your', 'them', 'theirs', 'who', 'of', 'ma', \"mightn't\", 'aren', 'o', \"didn't\", 'haven', 'being', 'did', 'you', 'mustn', 're', 'why', 'couldn', 'isn', 'this', 'its', 'some', 'not', 'doesn', \"you're\", 'has', 'nor', 'over', 'weren', \"should've\", 've', 'once', 'out', 'here', \"you'd\", 'an', 'hadn', \"wouldn't\", 'now', \"it's\", 'be', 'are', 'by', 'at', 'will', 'few', 'most', 'yourself', 'which', 'these', \"mustn't\", 'other', \"haven't\", 'against', 'below', 'from', 'too', 'into', 'because', 'to', 'with', 'each', \"wasn't\", 'how', 'they', 'same', 'been', 'can', 'what', 'as', 'whom', 'our', 'when', 'herself', 'mightn', \"aren't\", 'the', 'such', \"won't\", 'on', 'then', 'were', 'any', 'while', 'until', 'll', 'if', 'there', 'won', 'me', 'but', 'her', 'or', \"couldn't\", 'for', 'above', \"hasn't\", 'themselves', 'needn', 'had', \"she's\", 'having', 'y', 's', 'itself', 'off', \"hadn't\", \"shouldn't\", 'do', 'between', \"needn't\", 'have', 'don', \"weren't\", 'before', 'didn', 'than', 'i', 't', 'in', 'he', \"shan't\", 'no', 'she', 'is', 'about', \"that'll\"}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'soient', 'fut', 'qui', 'on', 'fût', 'sur', 'ou', 'd', 'seraient', 'serez', 'avons', 'étés', 'eussent', 'pour', 'été', 'étante', 'ayant', 'eûmes', 'sois', 'sommes', 'eus', 'ayez', 'est', 'ayantes', 'te', 'ait', 'mes', 'au', 'ne', 'étais', 't', 'm', 'sera', 'serait', 'eusses', 'aurons', 'suis', 'aurions', 'nous', 'vos', 'serions', 'auriez', 'seront', 'aie', 'ce', 'l', 'eues', 'pas', 'avais', 'seras', 'eûtes', 'ma', 'même', 'en', 'ayants', 'aura', 'ayons', 'étée', 'êtes', 'ont', 'c', 'aurez', 'aurait', 'des', 'ta', 'soyons', 'moi', 'fussent', 'lui', 'j', 'il', 'étaient', 'eusse', 's', 'était', 'y', 'je', 'serais', 'ton', 'aurais', 'les', 'soit', 'eue', 'elle', 'étées', 'du', 'seriez', 'serai', 'ayante', 'avions', 'avec', 'me', 'toi', 'à', 'aux', 'que', 'étantes', 'se', 'aies', 'le', 'de', 'leur', 'mais', 'eussions', 'avez', 'dans', 'serons', 'fûtes', 'eu', 'étions', 'furent', 'nos', 'fussiez', 'mon', 'tu', 'étant', 'vous', 'un', 'eux', 'es', 'qu', 'auront', 'ces', 'fusses', 'eut', 'n', 'eût', 'fûmes', 'sont', 'auras', 'notre', 'ils', 'sa', 'fusse', 'étiez', 'auraient', 'fussions', 'aient', 'avait', 'aurai', 'fus', 'aviez', 'avaient', 'ses', 'étants', 'par', 'la', 'votre', 'et', 'eussiez', 'soyez', 'as', 'tes', 'ai', 'eurent', 'une', 'son'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'لما', 'هي', 'ليستا', 'إنه', 'ولا', 'كيت', 'ذه', 'لوما', 'هن', 'إذا', 'مه', 'إليكما', 'هم', 'عسى', 'لاسيما', 'لست', 'كأن', 'نعم', 'عن', 'ذو', 'تلكم', 'أيها', 'كأنما', 'كأي', 'سوف', 'إليك', 'لن', 'كليكما', 'ليست', 'ليسا', 'إذن', 'ذلك', 'ذواتي', 'على', 'إلى', 'دون', 'هذان', 'لعل', 'بك', 'عليه', 'أن', 'هاتين', 'لسنا', 'ذوا', 'ذين', 'لستم', 'بكن', 'بس', 'عدا', 'لستن', 'فيما', 'قد', 'وما', 'بخ', 'بنا', 'حيث', 'ذاك', 'ذلكم', 'هلا', 'فيه', 'كل', 'أينما', 'اللذان', 'ذينك', 'له', 'تين', 'ها', 'أنى', 'أنتم', 'أولاء', 'إيه', 'هنا', 'ذا', 'ريث', 'هذه', 'هنالك', 'آه', 'التي', 'حتى', 'أين', 'لدى', 'هل', 'الذي', 'هو', 'وإذا', 'خلا', 'كي', 'ليت', 'سوى', 'هكذا', 'ذلكن', 'لكن', 'بلى', 'هذي', 'منه', 'عما', 'لكم', 'بهما', 'مذ', 'هاتان', 'أولئك', 'منها', 'ذات', 'ليسوا', 'كلتا', 'وهو', 'فإن', 'أما', 'إما', 'بكم', 'لي', 'لهما', 'إنا', 'شتان', 'عند', 'إلا', 'هاهنا', 'لولا', 'هذا', 'في', 'بماذا', 'حين', 'هؤلاء', 'وإن', 'كأين', 'ذانك', 'بعد', 'تي', 'لم', 'آها', 'تلك', 'لسن', 'بين', 'أنت', 'اللائي', 'حيثما', 'منذ', 'بل', 'كذا', 'أكثر', 'هيت', 'من', 'فيها', 'والذين', 'إليكم', 'هاته', 'أنا', 'مع', 'اللتين', 'ومن', 'يا', 'هذين', 'ممن', 'أو', 'كليهما', 'أي', 'أنتن', 'لكما', 'تينك', 'ذي', 'بهن', 'لستما', 'ذان', 'تلكما', 'بي', 'كيف', 'كذلك', 'عليك', 'فلا', 'فيم', 'هاتي', 'هاك', 'ته', 'أقل', 'بمن', 'اللتان', 'ذلكما', 'آي', 'متى', 'فمن', 'إذما', 'بيد', 'إليكن', 'اللذين', 'كلا', 'حاشا', 'لهم', 'لكيلا', 'هيا', 'لهن', 'حبذا', 'لكي', 'ليس', 'هيهات', 'عل', 'لك', 'فإذا', 'والذي', 'إنما', 'ما', 'أوه', 'إي', 'هناك', 'لا', 'أف', 'كم', 'أم', 'بما', 'نحن', 'ثمة', 'اللتيا', 'ألا', 'كلما', 'ذواتا', 'غير', 'إذ', 'كيفما', 'مهما', 'ثم', 'لو', 'بهم', 'لئن', 'ولكن', 'به', 'اللاتي', 'مما', 'الذين', 'ولو', 'بكما', 'نحو', 'وإذ', 'لها', 'اللواتي', 'أنتما', 'بها', 'ماذا', 'بعض', 'لكنما', 'لنا', 'كما', 'إن', 'هما', 'كلاهما'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(set(stopwords.words('arabic')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing (segmentation du texte)\n",
    "<p>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste des tokens :\n",
      "['In', 'this', 'tutorial', ',', 'You', \"'re\", 'learning', 'NLTK', '.', 'It', 'is', 'an', 'interesting', 'platform', '.', 'You', \"'re\", 'lucky']\n",
      "-------------------------------------\n",
      "Liste des tokens sans StopWords:\n",
      "['In', 'tutorial', ',', 'You', \"'re\", 'learning', 'NLTK', '.', 'It', 'interesting', 'platform', '.', 'You', \"'re\", 'lucky']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"In this tutorial, You're learning NLTK. It is an interesting platform. You're lucky\"\n",
    "\n",
    "words = word_tokenize(text, language='english')\n",
    "print(\"Liste des tokens :\")\n",
    "print(words)\n",
    "print(\"-------------------------------------\")\n",
    "new_sentence = []\n",
    " \n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        new_sentence.append(word)\n",
    "print(\"Liste des tokens sans StopWords:\") \n",
    "print(new_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[',', 'tutorial', 'learning', 'is', 'It', 'an', 'this', \"'re\", 'platform', 'You', 'NLTK', '.', 'In', 'lucky', 'interesting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"In this tutorial, You're learning NLTK. It is an interesting platform. You're lucky\"\n",
    "\n",
    "words = set(word_tokenize(text, language='english'))\n",
    " \n",
    "new_sentence = []\n",
    " \n",
    "for word in words:\n",
    "    new_sentence.append(word)\n",
    " \n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ceci est 1 première phrase.\n",
      "Puis j'en écris une seconde.\n",
      "pour finir en voilà une troisième sans mettre de majuscule\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    " \n",
    "\n",
    "text_fr = \"Ceci est 1 première phrase. Puis j'en écris une seconde. pour finir en voilà une troisième sans mettre de majuscule\"\n",
    " \n",
    "# We use the segmentation function on the text\n",
    "sentences = sent_tokenize(text_fr, language = 'french')\n",
    " \n",
    "# We print the sentences\n",
    "for sent in sentences:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "En linguistique, le stemming, la racinisation ou désuffixation est un procédé de transformation des mots en leur radical ou racine. La racine d’un mot correspond à la partie du mot restante une fois que l’on a supprimé son (ses) préfixe(s) et suffixe(s), afin d'obtenir son radical\n",
    "</p>\n",
    "<p>\n",
    "Deux principales familles d'algorithme de stemming sont présentes dans la littérature : orientés algorithmes et ceux utilisant un dictionnaire.\n",
    "<ul>\n",
    "<li><strong>Orienté algorithmes</strong> plus rapide et permet d'extraire des racines de mots inconnus.\n",
    "<li><strong>L'approche par dictionnaire</strong> quant à elle ne fait pas d'erreur sur les mots connus, mais en produit sur ceux qu'elle ne liste pas. Elle est aussi plus lente, et nécessite malgré tout la suppression de suffixes avant d'aller chercher la racine correspondante dans le dictionnaire.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ISRIStemmer',\n",
       " 'LancasterStemmer',\n",
       " 'PorterStemmer',\n",
       " 'RSLPStemmer',\n",
       " 'RegexpStemmer',\n",
       " 'SnowballStemmer',\n",
       " 'StemmerI',\n",
       " 'WordNetLemmatizer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'api',\n",
       " 'isri',\n",
       " 'lancaster',\n",
       " 'porter',\n",
       " 'regexp',\n",
       " 'rslp',\n",
       " 'snowball',\n",
       " 'util',\n",
       " 'wordnet']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Information sur nktl.stem : différents méthodes de stemming\n",
    "import nltk.stem\n",
    "dir(nltk.stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de stemming : porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movi', 'dog', 'plane', 'flower', 'fli', 'fri', 'fri', 'week', 'plant', 'run', 'throttl']\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted' ,'running','throttle']\n",
    "porter = stem.porter.PorterStemmer()\n",
    "p_words=[]\n",
    "for word in input_words:\n",
    "    p_words.append(porter.stem(word))\n",
    "print(p_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de steming : lancaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movy', 'dog', 'plan', 'flow', 'fli', 'fri', 'fry', 'week', 'plant', 'run', 'throttle']\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted' ,'running','throttle']\n",
    "lancaster = stem.lancaster.LancasterStemmer()\n",
    "l_words=[]\n",
    "for word in input_words:\n",
    "    l_words.append(lancaster.stem(word))\n",
    "print(l_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple de steming : snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movi', 'dog', 'plane', 'flower', 'fli', 'fri', 'fri', 'week', 'plant', 'run', 'throttl']\n"
     ]
    }
   ],
   "source": [
    "from nltk import stem\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks','planted' ,'running','throttle']\n",
    "snowball = stem.snowball.EnglishStemmer()\n",
    "s_words=[]\n",
    "for word in input_words:\n",
    "    s_words.append(snowball.stem(word))\n",
    "print(s_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatisation\n",
    "<p>\n",
    "La lemmatisation désigne l'analyse lexicale du contenu d'un texte regroupant les mots d'une même famille. Chacun des mots d'un contenu se trouve ainsi réduit en une entité appelée lemme (forme canonique). La lemmatisation regroupe les différentes formes que peut revêtir un mot, soit : le nom, le pluriel, le verbe à l'infinitif, etc.\n",
    "</p>\n",
    "<p>\n",
    "La lemmatisation d'une forme d'un mot consiste à en prendre sa forme canonique. Celle-ci est définie comme suit :\n",
    "<ul>\n",
    "<li>pour un verbe : ce verbe à l'infinitif,\n",
    "<li>pour les autres mots : le mot au masculin singulier.\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'dog', 'plane', 'flower', 'fly', 'fry', 'fry', 'week', 'planted', 'running', 'throttle', 'good', 'better', 'best', 'movie']\n"
     ]
    }
   ],
   "source": [
    "# N'oublier pas de télacharger nltk.download('wordnet')\n",
    "from nltk import stem\n",
    "\n",
    "input_words =['movies','dogs','planes','flowers','flies','fries','fry','weeks',\n",
    "'planted','running','throttle','good','better','best','movies']\n",
    "\n",
    "wordnet_lemm = stem.WordNetLemmatizer()\n",
    "words =[]\n",
    "for w in input_words:\n",
    "    words.append(stem.WordNetLemmatizer().lemmatize(w))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminer les stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tutorial', ',', \"'m\", 'learning', 'nltk', '.', 'interesting', 'platform', '.', 'good', 'luck']\n",
      "['tutorial', \"'m\", 'learning', 'nltk', 'interesting', 'platform', 'good', 'luck']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('English'))\n",
    "text = \"In this tutorial, I'm learning NLTK. It is an interesting platform. Good luck\".lower()\n",
    "\n",
    "words = word_tokenize(text, language='english')\n",
    " \n",
    "tokens = []\n",
    "result=[]\n",
    " \n",
    "for word in words:\n",
    "    if word not in stop_words:\n",
    "        tokens.append(word)\n",
    "for word in tokens:\n",
    "    if word not in string.punctuation:\n",
    "        result.append(word)\n",
    "print(tokens)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
